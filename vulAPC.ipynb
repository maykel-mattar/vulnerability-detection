{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vulnerability Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the required databases in neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "class DBManager:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.driver = GraphDatabase.driver(\"bolt://neo4j:7687\")\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def addDB(self, modes):\n",
    "        for modeitem in modes:\n",
    "            with self.driver.session() as session:\n",
    "                greeting = session.execute_write(self._addDBitem, modeitem)\n",
    "\n",
    "    @staticmethod\n",
    "    def _addDBitem(tx, modeitem):\n",
    "        result = tx.run(\"CREATE database \"\n",
    "                        \"$modeitem \", modeitem=modeitem)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    greeter = DBManager()\n",
    "    modes = ['ast','cfg','pdg','mixte','astcfg','astpdg','cfgpdg']\n",
    "    greeter.addDB(modes)\n",
    "    greeter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-28 17:26:43.667715: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-10-28 17:26:43.667740: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (maykel): /proc/driver/nvidia/version does not exist\n",
      "2022-10-28 17:26:43.679675: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import stellargraph as sg\n",
    "import numpy as np\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "from stellargraph.layer import DeepGraphCNN\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph import datasets\n",
    "import py2neo\n",
    "from stellargraph import StellarDiGraph\n",
    "from sklearn import model_selection\n",
    "from IPython.display import display, HTML\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the datasets in neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore',max_categories=10)\n",
    "def getDataSet(db):\n",
    "    default_host = os.environ.get(\"STELLARGRAPH_NEO4J_HOST\")\n",
    "    neo4j_graph = py2neo.Graph(host='neo4j', port=7687,name=db)\n",
    "    num_nodes = len(neo4j_graph.nodes)\n",
    "    num_relationships = len(neo4j_graph.relationships)\n",
    "    nodesLabelList = neo4j_graph\n",
    "    nodesLabelsraw =neo4j_graph.run(\"MATCH (n) RETURN distinct labels(n)[0] \")\n",
    "    raw_homogeneous_nodes = neo4j_graph.run(\n",
    "        ' MATCH (n) RETURN n.id AS id, n.line as line , toFloat(n.typeInt) as type, labels(n)[0] as nodeTypeLabel'\n",
    "    )\n",
    "    raw_homogeneous_nodesUpdated=[]\n",
    "    for item in raw_homogeneous_nodes:\n",
    "        raw_homogeneous_nodesUpdated.append({ \"id\":item['id'],\n",
    "                                              'line':item['line'],\n",
    "                                               'nodeTypeLabel':item['nodeTypeLabel'] if item['nodeTypeLabel']  else 'default'\n",
    "                                             })\n",
    "    raw_homogeneous_nodesUpdated = pd.DataFrame(raw_homogeneous_nodesUpdated)\n",
    "    encoder_df = pd.DataFrame(encoder.fit_transform(raw_homogeneous_nodesUpdated[['nodeTypeLabel']]).toarray())\n",
    "    final_df = raw_homogeneous_nodesUpdated.join(encoder_df)\n",
    "    final_df.drop('nodeTypeLabel', axis=1, inplace=True)\n",
    "    homogeneous_nodes = final_df.set_index(\"id\")\n",
    "    edges = neo4j_graph.run(\n",
    "        \"\"\"\n",
    "        MATCH (s) -[r]-> (t)\n",
    "        RETURN s.id AS source, t.id AS target, type(r) as type\n",
    "        \"\"\"\n",
    "    ).to_data_frame()\n",
    "    edges.head()\n",
    "    homogeneous = StellarDiGraph(homogeneous_nodes, edges,edge_type_column=\"type\")\n",
    "    return homogeneous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllLabels(filePath):\n",
    "    files = os.listdir(filePath)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "DataLoading"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "**************************                            **************************\n",
      "**************************   Representaion Importer   **************************\n",
      "**************************                            **************************\n",
      "********************************************************************************\n",
      "\n",
      "Extracting Representations |████████████████████████████████████████| 3/3 [100%] in 1.5s (2.02/s)\n",
      "Mixed Import |████████████████████████████████████████| 39/39 [100%] in 1.4s (27.41/s)\n",
      "16 Nodes are inserted\n",
      "15 Relations are inserted\n",
      "0 Flows are inserted\n",
      "7 Controls are inserted\n",
      "\n",
      "********************************************************************************\n",
      "**************************                            **************************\n",
      "**************************   Representaion Importer   **************************\n",
      "**************************                            **************************\n",
      "********************************************************************************\n",
      "\n",
      "Extracting Representations |████████████████████████████████████████| 3/3 [100%] in 1.6s (1.92/s)\n",
      "Mixed Import |████████████████████████████████████████| 39/39 [100%] in 0.2s (159.43/s)\n",
      "16 Nodes are inserted\n",
      "15 Relations are inserted\n",
      "0 Flows are inserted\n",
      "7 Controls are inserted\n",
      "\n",
      "********************************************************************************\n",
      "**************************                            **************************\n",
      "**************************   Representaion Importer   **************************\n",
      "**************************                            **************************\n",
      "********************************************************************************\n",
      "\n",
      "Extracting Representations |████████████████████████████████████████| 3/3 [100%] in 1.6s (1.90/s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m/home/maykel/vulnerability_detector/vulAPC.ipynb Cell 9\u001B[0m in \u001B[0;36m<cell line: 9>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     <a href='vscode-notebook-cell:/home/maykel/vulnerability_detector/vulAPC.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001B[0m javaFilePath \u001B[39m=\u001B[39m dirPath\u001B[39m+\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m/\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m+\u001B[39mfile\u001B[39m+\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m/\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m+\u001B[39msubFileItem\u001B[39m+\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m/\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m+\u001B[39mcodeFileItem\u001B[39m+\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m/before/GenericClass.java\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m     <a href='vscode-notebook-cell:/home/maykel/vulnerability_detector/vulAPC.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001B[0m shutil\u001B[39m.\u001B[39mcopy(javaFilePath, \u001B[39m\"\u001B[39m\u001B[39m./\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[0;32m---> <a href='vscode-notebook-cell:/home/maykel/vulnerability_detector/vulAPC.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001B[0m subprocess\u001B[39m.\u001B[39;49mcall([\u001B[39m'\u001B[39;49m\u001B[39mpython3\u001B[39;49m\u001B[39m'\u001B[39;49m, \u001B[39m'\u001B[39;49m\u001B[39m./scripts/Importer.py\u001B[39;49m\u001B[39m'\u001B[39;49m, \u001B[39m'\u001B[39;49m\u001B[39m-f\u001B[39;49m\u001B[39m'\u001B[39;49m,\u001B[39m'\u001B[39;49m\u001B[39mGenericClass.java\u001B[39;49m\u001B[39m'\u001B[39;49m, \u001B[39m'\u001B[39;49m\u001B[39m-m\u001B[39;49m\u001B[39m'\u001B[39;49m,mode])\n\u001B[1;32m     <a href='vscode-notebook-cell:/home/maykel/vulnerability_detector/vulAPC.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001B[0m datasetArray\u001B[39m.\u001B[39mappend(getDataSet(mode))\n\u001B[1;32m     <a href='vscode-notebook-cell:/home/maykel/vulnerability_detector/vulAPC.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001B[0m yaArr[labelsArr\u001B[39m.\u001B[39mindex(file)]\u001B[39m=\u001B[39m\u001B[39m1\u001B[39m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/subprocess.py:342\u001B[0m, in \u001B[0;36mcall\u001B[0;34m(timeout, *popenargs, **kwargs)\u001B[0m\n\u001B[1;32m    340\u001B[0m \u001B[39mwith\u001B[39;00m Popen(\u001B[39m*\u001B[39mpopenargs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs) \u001B[39mas\u001B[39;00m p:\n\u001B[1;32m    341\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 342\u001B[0m         \u001B[39mreturn\u001B[39;00m p\u001B[39m.\u001B[39;49mwait(timeout\u001B[39m=\u001B[39;49mtimeout)\n\u001B[1;32m    343\u001B[0m     \u001B[39mexcept\u001B[39;00m:  \u001B[39m# Including KeyboardInterrupt, wait handled that.\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         p\u001B[39m.\u001B[39mkill()\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/subprocess.py:1079\u001B[0m, in \u001B[0;36mPopen.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1077\u001B[0m     endtime \u001B[39m=\u001B[39m _time() \u001B[39m+\u001B[39m timeout\n\u001B[1;32m   1078\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m-> 1079\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_wait(timeout\u001B[39m=\u001B[39;49mtimeout)\n\u001B[1;32m   1080\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m   1081\u001B[0m     \u001B[39m# https://bugs.python.org/issue25942\u001B[39;00m\n\u001B[1;32m   1082\u001B[0m     \u001B[39m# The first keyboard interrupt waits briefly for the child to\u001B[39;00m\n\u001B[1;32m   1083\u001B[0m     \u001B[39m# exit under the common assumption that it also received the ^C\u001B[39;00m\n\u001B[1;32m   1084\u001B[0m     \u001B[39m# generated SIGINT and will exit rapidly.\u001B[39;00m\n\u001B[1;32m   1085\u001B[0m     \u001B[39mif\u001B[39;00m timeout \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/subprocess.py:1804\u001B[0m, in \u001B[0;36mPopen._wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1802\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mreturncode \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m   1803\u001B[0m     \u001B[39mbreak\u001B[39;00m  \u001B[39m# Another thread waited.\u001B[39;00m\n\u001B[0;32m-> 1804\u001B[0m (pid, sts) \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_try_wait(\u001B[39m0\u001B[39;49m)\n\u001B[1;32m   1805\u001B[0m \u001B[39m# Check the pid and loop as waitpid has been known to\u001B[39;00m\n\u001B[1;32m   1806\u001B[0m \u001B[39m# return 0 even without WNOHANG in odd situations.\u001B[39;00m\n\u001B[1;32m   1807\u001B[0m \u001B[39m# http://bugs.python.org/issue14396.\u001B[39;00m\n\u001B[1;32m   1808\u001B[0m \u001B[39mif\u001B[39;00m pid \u001B[39m==\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpid:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/subprocess.py:1762\u001B[0m, in \u001B[0;36mPopen._try_wait\u001B[0;34m(self, wait_flags)\u001B[0m\n\u001B[1;32m   1760\u001B[0m \u001B[39m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001B[39;00m\n\u001B[1;32m   1761\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m-> 1762\u001B[0m     (pid, sts) \u001B[39m=\u001B[39m os\u001B[39m.\u001B[39;49mwaitpid(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mpid, wait_flags)\n\u001B[1;32m   1763\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mChildProcessError\u001B[39;00m:\n\u001B[1;32m   1764\u001B[0m     \u001B[39m# This happens if SIGCLD is set to be ignored or waiting\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m     \u001B[39m# for child processes has otherwise been disabled for our\u001B[39;00m\n\u001B[1;32m   1766\u001B[0m     \u001B[39m# process.  This child is dead, we can't get the status.\u001B[39;00m\n\u001B[1;32m   1767\u001B[0m     pid \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpid\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "datasetArray = []\n",
    "labelsArray = []\n",
    "dirPath = \"./juliet2\"\n",
    "files = os.listdir(dirPath)\n",
    "labelsArr = (getAllLabels(dirPath))\n",
    "\n",
    "mode = 'mixte'\n",
    "for file in files:\n",
    "    subFile = os.listdir(dirPath+\"/\"+file)\n",
    "    vulName =file[:file.find('_')]\n",
    "    if vulName not in labels:\n",
    "        labels.append(vulName)\n",
    "    for subFileItem in subFile:\n",
    "        codeFile = os.listdir(dirPath+\"/\"+file+\"/\"+subFileItem)\n",
    "        for codeFileItem in codeFile:\n",
    "            yaArr = np.zeros(len(labelsArr), dtype=int)\n",
    "            javaFilePath = dirPath+\"/\"+file+\"/\"+subFileItem+\"/\"+codeFileItem+\"/before/GenericClass.java\"\n",
    "            shutil.copy(javaFilePath, \"./\")\n",
    "            subprocess.call(['python3', './scripts/Importer.py', '-f','GenericClass.java', '-m',mode],    stdout=subprocess.DEVNULL,stderr=subprocess.STDOUT)\n",
    "            datasetArray.append(getDataSet(mode))\n",
    "            yaArr[labelsArr.index(file)]=1\n",
    "            labelsArray.append(yaArr.tolist())\n",
    "            os.remove(\"GenericClass.java\")\n",
    "            yaArr = np.zeros(len(labelsArr), dtype=int)\n",
    "            javaFilePath = dirPath+\"/\"+file+\"/\"+subFileItem+\"/\"+codeFileItem+\"/after/GenericClass.java\"\n",
    "            shutil.copy(javaFilePath, \"./\")\n",
    "            subprocess.call(['python3', './scripts/Importer.py', '-f','GenericClass.java', '-m',mode],    stdout=subprocess.DEVNULL,stderr=subprocess.STDOUT)\n",
    "            datasetArray.append(getDataSet(mode))\n",
    "            labelsArray.append(yaArr.tolist())\n",
    "            os.remove(\"GenericClass.java\")\n",
    "    print(file)\n",
    "graphs =datasetArray\n",
    "graph_labels = labelsArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics of the sizes of the graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame(\n",
    "    [(g.number_of_nodes(), g.number_of_edges()) for g in graphs],\n",
    "    columns=[\"nodes\", \"edges\"],\n",
    ")\n",
    "summary.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are array of binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_labelsraw = pd.DataFrame(labelsArray)\n",
    "graph_labelsraw.value_counts().to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_labels = pd.get_dummies(graph_labelsraw, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare graph generator\n",
    "\n",
    "To feed data to the `tf.Keras` model that we will create later, we need a data generator. For supervised graph classification, we create an instance of `StellarGraph`'s `PaddedGraphGenerator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = PaddedGraphGenerator(graphs=graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 35  # the number of rows for the output tensor\n",
    "layer_sizes = [32, 32, 32, 4]\n",
    "\n",
    "dgcnn_model = DeepGraphCNN(\n",
    "    layer_sizes=layer_sizes,\n",
    "    activations=[\"tanh\", \"tanh\", \"tanh\", \"tanh\"],\n",
    "    k=k,\n",
    "    bias=False,\n",
    "    generator=generator,\n",
    ")\n",
    "x_inp, x_out = dgcnn_model.in_out_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add the convolutional, max pooling, and dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes))(x_out)\n",
    "x_out = MaxPool1D(pool_size=2)(x_out)\n",
    "\n",
    "x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)\n",
    "\n",
    "x_out = Flatten()(x_out)\n",
    "\n",
    "\n",
    "x_out = Dense(units=128, activation=\"relu\")(x_out)\n",
    "\n",
    "x_out = Dense(units=64, activation=\"relu\")(x_out)\n",
    "\n",
    "x_out = Dropout(rate=0.5)(x_out)\n",
    "\n",
    "output1 = Dense(len(labelsArr), activation='sigmoid')(x_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the `Keras` model and prepare it for training by specifying the loss and optimisation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=x_inp, outputs=output1)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.00001), loss=tf.keras.losses.binary_crossentropy,metrics=[tf.metrics.binary_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "We can now train the model using the model's `fit` method.\n",
    "\n",
    "But first we need to split our data to training and test sets. We are going to use 90% of the data for training and the remaining 10% for testing. This 90/10 split is the equivalent of a single fold in the 10-fold cross validation scheme used in [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs, test_graphs = model_selection.train_test_split(\n",
    "    graph_labels, train_size=0.6,shuffle=True, stratify=graph_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data split into train and test sets, we create a `StellarGraph.PaddedGenerator` generator object that prepares the data for training. We create data generators suitable for training at `tf.keras` model by calling the latter generator's `flow` method specifying the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = PaddedGraphGenerator(graphs=graphs)\n",
    "\n",
    "train_gen = gen.flow(\n",
    "    list(train_graphs.index - 1),\n",
    "    targets=tf.convert_to_tensor(train_graphs.values),\n",
    "    symmetric_normalization=False,\n",
    ")\n",
    "test_gen = gen.flow(\n",
    "    list(test_graphs.index - 1),\n",
    "    targets=tf.convert_to_tensor(test_graphs.values),\n",
    "    symmetric_normalization=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We set the number of epochs to a large value so the call to `model.fit(...)` later might take a long time to complete. For faster performance set `epochs` to a smaller value; but if you do accuracy of the model found may be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model by calling it's `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the training history (losses and accuracies for the train and test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = sg.utils.plot_history(history,return_figure=True)\n",
    "pl.savefig(\"history-\"+mode+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us calculate the performance of the trained model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "with open('res'+mode+'binary.txt', 'w') as f:\n",
    "    for name, val in zip(model.metrics_names, test_metrics):\n",
    "        f.write(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "        f.write('\\n')\n",
    "\n",
    "        print(\"\\t{}: {:0.4f}\".format(name, val))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('3.8.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "fca9ada9be7055bbb37cdeb1d76d1422c3da7716e11e132375b6710e22e1f379"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
